{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering valid files...\n",
      "Filtering valid files: 100%|██████████| 41/41 [06:10<00:00,  9.03s/it]\n",
      "Finding common columns across valid files...\n",
      "Finding common columns: 100%|██████████| 39/39 [06:14<00:00,  9.60s/it]\n",
      "Processing and saving files...\n",
      "Processing files:   0%|          | 0/39 [00:00<?, ?it/s]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE134429.feather\n",
      "Processing files:   3%|▎         | 1/39 [00:30<19:25, 30.68s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE131752.feather\n",
      "Processing files:   5%|▌         | 2/39 [01:01<19:03, 30.90s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE87640.feather\n",
      "Processing files:   8%|▊         | 3/39 [01:27<16:59, 28.33s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE217633.feather\n",
      "Processing files:  10%|█         | 4/39 [02:01<17:53, 30.67s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE87648.feather\n",
      "Processing files:  13%|█▎        | 5/39 [02:27<16:22, 28.90s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE81961.feather\n",
      "Processing files:  15%|█▌        | 6/39 [02:52<15:14, 27.70s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE59685.feather\n",
      "Processing files:  18%|█▊        | 7/39 [03:17<14:24, 27.01s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE144858.feather\n",
      "Processing files:  21%|██        | 8/39 [03:42<13:29, 26.13s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE99624.feather\n",
      "Processing files:  23%|██▎       | 9/39 [04:07<12:58, 25.95s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE118468.feather\n",
      "Processing files:  26%|██▌       | 10/39 [04:32<12:22, 25.59s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE106648.feather\n",
      "Processing files:  28%|██▊       | 11/39 [04:59<12:06, 25.94s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE111223.feather\n",
      "Processing files:  31%|███       | 12/39 [05:25<11:40, 25.93s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE67751.feather\n",
      "Processing files:  33%|███▎      | 13/39 [05:51<11:13, 25.91s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE219293.feather\n",
      "Processing files:  36%|███▌      | 14/39 [06:22<11:29, 27.59s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE122244.feather\n",
      "Processing files:  38%|███▊      | 15/39 [06:54<11:31, 28.80s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE130029.feather\n",
      "Processing files:  41%|████      | 16/39 [07:19<10:37, 27.73s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE77696.feather\n",
      "Processing files:  44%|████▎     | 17/39 [07:45<09:59, 27.24s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE130030.feather\n",
      "Processing files:  46%|████▌     | 18/39 [08:11<09:23, 26.83s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE42861.feather\n",
      "Processing files:  49%|████▊     | 19/39 [08:38<08:57, 26.90s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE193836.feather\n",
      "Processing files:  51%|█████▏    | 20/39 [09:02<08:17, 26.19s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE71841.feather\n",
      "Processing files:  54%|█████▍    | 21/39 [09:27<07:41, 25.64s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE145714.feather\n",
      "Processing files:  56%|█████▋    | 22/39 [09:59<07:47, 27.51s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE72776.feather\n",
      "Processing files:  59%|█████▉    | 23/39 [10:24<07:11, 26.98s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE72338.feather\n",
      "Processing files:  62%|██████▏   | 24/39 [10:49<06:34, 26.31s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE72774.feather\n",
      "Processing files:  64%|██████▍   | 25/39 [11:17<06:14, 26.72s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE175364.feather\n",
      "Processing files:  67%|██████▋   | 26/39 [11:42<05:39, 26.13s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE118469.feather\n",
      "Processing files:  69%|██████▉   | 27/39 [12:07<05:11, 25.93s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE166611.feather\n",
      "Processing files:  72%|███████▏  | 28/39 [12:31<04:37, 25.20s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE131989.feather\n",
      "Processing files:  74%|███████▍  | 29/39 [12:56<04:13, 25.34s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE67705.feather\n",
      "Processing files:  77%|███████▋  | 30/39 [13:23<03:50, 25.62s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE32148.feather\n",
      "Processing files:  79%|███████▉  | 31/39 [13:48<03:23, 25.47s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE182991.feather\n",
      "Processing files:  82%|████████▏ | 32/39 [14:19<03:09, 27.13s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE111629.feather\n",
      "Processing files:  85%|████████▍ | 33/39 [14:46<02:43, 27.30s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE56581.feather\n",
      "Processing files:  87%|████████▋ | 34/39 [15:12<02:13, 26.71s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE107143.feather\n",
      "Processing files:  90%|████████▉ | 35/39 [15:37<01:44, 26.23s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE56046.feather\n",
      "Processing files:  92%|█████████▏| 36/39 [16:07<01:22, 27.34s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE156994.feather\n",
      "Processing files:  95%|█████████▍| 37/39 [16:32<00:53, 26.72s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE143942.feather\n",
      "Processing files:  97%|█████████▋| 38/39 [16:57<00:26, 26.10s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE214297.feather\n",
      "Processing files: 100%|██████████| 39/39 [17:23<00:00, 26.75s/it]\n"
     ]
    }
   ],
   "source": [
    "###here I remove too small datasets with 27k sites and filter the remain data to include only those sites which located in all datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def process_feather_files(input_folder, output_folder, min_columns=28000):\n",
    "    feather_files = [f for f in os.listdir(input_folder) if f.endswith('.feather')]\n",
    "    valid_files = []\n",
    "    \n",
    "    print(\"Filtering valid files...\")\n",
    "    for file in tqdm(feather_files, desc=\"Filtering valid files\", file=sys.stdout):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        df = pd.read_feather(file_path)\n",
    "        if df.shape[1] >= min_columns:\n",
    "            valid_files.append(file_path)\n",
    "\n",
    "    if not valid_files:\n",
    "        print(\"No valid files found with the minimum column requirement.\")\n",
    "        return\n",
    "\n",
    "    common_columns = None\n",
    "    print(\"Finding common columns across valid files...\")\n",
    "    for file_path in tqdm(valid_files, desc=\"Finding common columns\", file=sys.stdout):\n",
    "        df = pd.read_feather(file_path)\n",
    "        if common_columns is None:\n",
    "            common_columns = set(df.columns)\n",
    "        else:\n",
    "            common_columns.intersection_update(df.columns)\n",
    "\n",
    "    if not common_columns:\n",
    "        print(\"No common columns found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\"Processing and saving files...\")\n",
    "    for file_path in tqdm(valid_files, desc=\"Processing files\", file=sys.stdout):\n",
    "        df = pd.read_feather(file_path)\n",
    "        df_common = df[list(common_columns)]\n",
    "        \n",
    "        output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "        df_common.to_feather(output_path)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "input_folder = \"/home/vpal/hobotnica/All_datasets/datasets/data_feather_no_age_big_ds\"\n",
    "output_folder = \"/home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception\"\n",
    "process_feather_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed GSE118469, current unique sites: 170691\n",
      "Processed GSE59685, current unique sites: 238824\n",
      "Processed GSE111629, current unique sites: 296757\n",
      "Processed GSE134429, current unique sites: 301989\n",
      "Processed GSE32148, current unique sites: 307124\n",
      "Processed GSE118468, current unique sites: 317649\n",
      "Processed GSE56581, current unique sites: 318620\n",
      "Processed GSE130030, current unique sites: 321960\n",
      "Processed GSE214297, current unique sites: 323269\n",
      "Processed GSE42861, current unique sites: 323698\n",
      "Processed GSE193836, current unique sites: 328408\n",
      "Processed GSE87640, current unique sites: 328413\n",
      "Processed GSE143942, current unique sites: 328414\n",
      "Processed GSE107143, current unique sites: 332075\n",
      "Processed GSE56606, current unique sites: 332184\n",
      "Processed GSE77696, current unique sites: 335328\n",
      "Processed GSE72338, current unique sites: 335916\n",
      "Processed GSE106648, current unique sites: 335916\n",
      "Processed GSE72774, current unique sites: 335966\n",
      "Processed GSE99624, current unique sites: 336403\n",
      "Processed GSE87648, current unique sites: 337740\n",
      "Processed GSE217633, current unique sites: 342569\n",
      "Processed GSE175364, current unique sites: 342569\n",
      "Processed GSE145714, current unique sites: 342718\n",
      "Processed GSE81961, current unique sites: 342718\n",
      "Processed GSE56046, current unique sites: 342718\n",
      "Processed GSE122244, current unique sites: 343868\n",
      "Processed GSE131752, current unique sites: 343980\n",
      "Processed GSE49909, current unique sites: 344071\n",
      "Processed GSE111223, current unique sites: 344099\n",
      "Processed GSE67705, current unique sites: 344099\n",
      "Processed GSE144858, current unique sites: 344649\n",
      "Processed GSE67751, current unique sites: 344649\n",
      "Processed GSE130029, current unique sites: 344972\n",
      "Processed GSE71841, current unique sites: 344972\n",
      "Processed GSE156994, current unique sites: 345497\n",
      "Processed GSE219293, current unique sites: 345573\n",
      "Processed GSE72776, current unique sites: 345601\n",
      "Processed GSE131989, current unique sites: 346877\n",
      "Processed GSE182991, current unique sites: 346924\n",
      "Processed GSE166611, current unique sites: 347252\n",
      "Common sites found in at least 70.0% of datasets saved to /home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/common_sites_threshold.txt\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def extract_unique_sites(file_path):\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path, sep='\\t', usecols=[0, 1, 2], engine='c', low_memory=False)\n",
    "#         return set(df.iloc[:, 0]).union(set(df.iloc[:, 1]), set(df.iloc[:, 2]))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\")\n",
    "#         return set()\n",
    "\n",
    "# def find_common_sites(folder_path, output_file, threshold=1.0):\n",
    "#     files = [f for f in os.listdir(folder_path)]\n",
    "\n",
    "#     site_counts = {}\n",
    "#     total_files = len(files)\n",
    "\n",
    "#     for filename in files:\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         unique_sites = extract_unique_sites(file_path)\n",
    "        \n",
    "#         for site in unique_sites:\n",
    "#             if site in site_counts:\n",
    "#                 site_counts[site] += 1\n",
    "#             else:\n",
    "#                 site_counts[site] = 1\n",
    "\n",
    "#         print(f\"Processed {filename}, current unique sites: {len(site_counts)}\")\n",
    "\n",
    "#     min_occurrences = int(threshold * total_files)\n",
    "\n",
    "#     common_sites = [site for site, count in site_counts.items() if count >= min_occurrences]\n",
    "#     if common_sites:\n",
    "#         with open(output_file, 'w') as f:\n",
    "#             for site in sorted(common_sites):\n",
    "#                 f.write(f\"{site}\\n\")\n",
    "#         print(f\"Common sites found in at least {threshold*100}% of datasets saved to {output_file}\")\n",
    "#     else:\n",
    "#         print(f\"No common sites found in at least {threshold*100}% of datasets.\")\n",
    "\n",
    "# folder_path = '/home/vpal/hobotnica/All_datasets/triplets_h_score_bigger_than_0.5'\n",
    "# output_file = '/home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/common_sites_threshold.txt'\n",
    "\n",
    "# find_common_sites(folder_path, output_file, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def visualize_overlap_matrix(csv_file, output_image):\n",
    "#     \"\"\"Visualize the overlap matrix as a heatmap without annotations.\"\"\"\n",
    "#     # Load the overlap matrix from the CSV file\n",
    "#     overlap_matrix = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "#     # Create a heatmap using Seaborn without annotations\n",
    "#     plt.figure(figsize=(18, 15))\n",
    "#     sns.heatmap(\n",
    "#         overlap_matrix, \n",
    "#         annot=False,    # Disable annotations\n",
    "#         cmap='viridis', \n",
    "#         linewidths=0.3, \n",
    "#         square=True, \n",
    "#         cbar_kws={'label': 'Number of Common Columns'}\n",
    "#     )\n",
    "    \n",
    "#     # Add titles and labels\n",
    "#     plt.title(\"Column Overlap Between Datasets\", fontsize=16)\n",
    "#     plt.xlabel(\"Datasets\")\n",
    "#     plt.ylabel(\"Datasets\")\n",
    "    \n",
    "#     # Rotate x-axis labels for better readability\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.yticks(rotation=0)\n",
    "    \n",
    "#     # Save the heatmap as an image file\n",
    "#     plt.savefig(output_image, format='png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "# # Define the paths to your CSV file and the output image\n",
    "# csv_file = '/home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/overlap_matrix.csv'\n",
    "# output_image = '/home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/overlap_heatmap_no_numbers.png'\n",
    "\n",
    "# # Visualize the overlap matrix\n",
    "# visualize_overlap_matrix(csv_file, output_image)\n",
    "# print(f\"Heatmap saved as {output_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def count_site_occurrences(folder_path):\n",
    "#     site_counts = defaultdict(int)\n",
    "\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         df = pd.read_csv(file_path, sep='\\t', engine='python')\n",
    "\n",
    "#         all_sites = set(df['Column1']).union(set(df['Column2']), set(df['Column3']))\n",
    "#         for site in all_sites:\n",
    "#             site_counts[site] += 1\n",
    "\n",
    "#     return site_counts\n",
    "\n",
    "# def filter_rows_by_site_count(folder_path, site_counts, min_threshold=20):\n",
    "\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         df = pd.read_csv(file_path, sep='\\t', engine='python')\n",
    "        \n",
    "#         filtered_df = df[\n",
    "#             (df['Column1'].apply(lambda x: site_counts[x] >= min_threshold)) &\n",
    "#             (df['Column2'].apply(lambda x: site_counts[x] >= min_threshold)) &\n",
    "#             (df['Column3'].apply(lambda x: site_counts[x] >= min_threshold))\n",
    "#         ]\n",
    "\n",
    "#         filtered_df.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "# folder_path = '/home/vpal/hobotnica/All_datasets/triplets_h_score_bigger_than_0.5'\n",
    "\n",
    "# site_counts = count_site_occurrences(folder_path)\n",
    "\n",
    "# filter_rows_by_site_count(folder_path, site_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed GSE118469, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE118469\n",
      "Processed GSE59685, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE59685\n",
      "Processed GSE111629, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE111629\n",
      "Processed GSE134429, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE134429\n",
      "Processed GSE32148, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE32148\n",
      "Processed GSE118468, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE118468\n",
      "Processed GSE56581, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE56581\n",
      "Processed GSE130030, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE130030\n",
      "Processed GSE214297, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE214297\n",
      "Processed GSE42861, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE42861\n",
      "Processed GSE193836, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE193836\n",
      "Processed GSE87640, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE87640\n",
      "Processed GSE143942, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE143942\n",
      "Processed GSE107143, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE107143\n",
      "Processed GSE56606, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE56606\n",
      "Processed GSE77696, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE77696\n",
      "Processed GSE72338, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE72338\n",
      "Processed GSE106648, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE106648\n",
      "Processed GSE72774, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE72774\n",
      "Processed GSE99624, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE99624\n",
      "Processed GSE87648, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE87648\n",
      "Processed GSE217633, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE217633\n",
      "Processed GSE175364, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE175364\n",
      "Processed GSE145714, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE145714\n",
      "Processed GSE81961, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE81961\n",
      "Processed GSE56046, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE56046\n",
      "Processed GSE122244, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE122244\n",
      "Processed GSE131752, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE131752\n",
      "Processed GSE49909, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE49909\n",
      "Processed GSE111223, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE111223\n",
      "Processed GSE67705, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE67705\n",
      "Processed GSE144858, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE144858\n",
      "Processed GSE67751, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE67751\n",
      "Processed GSE130029, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE130029\n",
      "Processed GSE71841, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE71841\n",
      "Processed GSE156994, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE156994\n",
      "Processed GSE219293, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE219293\n",
      "Processed GSE72776, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE72776\n",
      "Processed GSE131989, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE131989\n",
      "Processed GSE182991, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE182991\n",
      "Processed GSE166611, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE166611\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def filter_files(input_folder, output_folder):\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "#     # Iterate through all .txt files in the input folder\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         input_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "#         # Read the file into a DataFrame\n",
    "#         df = pd.read_csv(input_path, sep='\\s+', engine='python')\n",
    "        \n",
    "#         # Filter out rows where the h-score (4th column) is less than 0.5\n",
    "#         filtered_df = df[df['Result'] >= 0.5]\n",
    "        \n",
    "#         # Save the filtered DataFrame to the output folder\n",
    "#         output_path = os.path.join(output_folder, filename)\n",
    "#         filtered_df.to_csv(output_path, sep='\\t', index=False)\n",
    "#         print(f\"Processed {filename}, saved to {output_path}\")\n",
    "\n",
    "# # Define the input and output folders\n",
    "# input_folder = '/home/vpal/hobotnica/All_datasets/H_scores_triplets'\n",
    "# output_folder = '/home/vpal/hobotnica/All_datasets/triplets_h_score_bigger'\n",
    "\n",
    "# # Run the function\n",
    "# filter_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/41 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|▏         | 1/41 [00:13<09:14, 13.87s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE134429.feather with 766019 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   5%|▍         | 2/41 [00:28<09:17, 14.31s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE131752.feather with 816982 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 3/41 [00:37<07:29, 11.84s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE87640.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  10%|▉         | 4/41 [00:53<08:21, 13.54s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE217633.feather with 835168 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  12%|█▏        | 5/41 [01:02<07:05, 11.82s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE87648.feather with 460305 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  15%|█▍        | 6/41 [01:10<06:15, 10.72s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE81961.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  17%|█▋        | 7/41 [01:19<05:42, 10.07s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE59685.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|█▉        | 8/41 [01:27<05:06,  9.28s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE144858.feather with 410891 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  22%|██▏       | 9/41 [01:35<04:50,  9.09s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE99624.feather with 485478 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  24%|██▍       | 10/41 [01:44<04:39,  9.00s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE118468.feather with 485479 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  27%|██▋       | 11/41 [01:53<04:30,  9.02s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE106648.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  29%|██▉       | 12/41 [02:02<04:21,  9.02s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE111223.feather with 485479 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  32%|███▏      | 13/41 [02:11<04:11,  8.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE67751.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  34%|███▍      | 14/41 [02:27<04:56, 10.99s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE219293.feather with 865768 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  37%|███▋      | 15/41 [02:42<05:19, 12.28s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE122244.feather with 822800 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  39%|███▉      | 16/41 [02:51<04:41, 11.25s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE130029.feather with 469664 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  41%|████▏     | 17/41 [03:00<04:15, 10.64s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE77696.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  44%|████▍     | 18/41 [03:09<03:51, 10.06s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE130030.feather with 471338 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  46%|████▋     | 19/41 [03:21<03:54, 10.65s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE42861.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  49%|████▉     | 20/41 [03:29<03:25,  9.80s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE193836.feather with 414249 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  51%|█████     | 21/41 [03:37<03:09,  9.46s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE71841.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  54%|█████▎    | 22/41 [03:53<03:37, 11.43s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE145714.feather with 865999 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  56%|█████▌    | 23/41 [04:02<03:11, 10.64s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE72776.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  59%|█████▊    | 24/41 [04:11<02:50, 10.04s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE72338.feather with 485476 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  61%|██████    | 25/41 [04:20<02:38,  9.88s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE72774.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  63%|██████▎   | 26/41 [04:29<02:23,  9.59s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE175364.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  66%|██████▌   | 27/41 [04:38<02:11,  9.38s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE118469.feather with 485481 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  68%|██████▊   | 28/41 [04:46<01:54,  8.84s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE166611.feather with 409889 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  71%|███████   | 29/41 [04:55<01:46,  8.87s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE131989.feather with 407514 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  73%|███████▎  | 30/41 [05:04<01:38,  8.94s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE67705.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  76%|███████▌  | 31/41 [05:13<01:29,  8.94s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE32148.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  78%|███████▊  | 32/41 [05:28<01:37, 10.85s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE182991.feather with 825171 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  80%|████████  | 33/41 [05:40<01:28, 11.07s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE111629.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  83%|████████▎ | 34/41 [05:49<01:13, 10.46s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE56581.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  85%|████████▌ | 35/41 [05:49<00:45,  7.51s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE56606.feather with 27580 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 36/41 [05:57<00:38,  7.74s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE107143.feather with 460297 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|█████████ | 37/41 [06:08<00:34,  8.58s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE56046.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  93%|█████████▎| 38/41 [06:16<00:25,  8.54s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE156994.feather with 403356 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▌| 39/41 [06:17<00:12,  6.17s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE49909.feather with 27580 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  98%|█████████▊| 40/41 [06:26<00:06,  7.00s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE143942.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 41/41 [06:36<00:00,  9.66s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE214297.feather with 518476 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Columns across all datasets: 15300\n",
      "Unique Columns across all datasets: 900354\n",
      "\n",
      "Calculating column overlap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating overlaps: 820pair [01:01, 13.26pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis completed. Results saved as 'column_analysis.txt' and 'column_overlap.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_common_columns(datasets):\n",
    "    common_columns = set(datasets[0])\n",
    "    for columns in datasets[1:]:\n",
    "        common_columns.intersection_update(columns)\n",
    "    return common_columns\n",
    "\n",
    "def get_all_columns(datasets):\n",
    "    all_columns = set()\n",
    "    for columns in datasets:\n",
    "        all_columns.update(columns)\n",
    "    return all_columns\n",
    "\n",
    "def analyze_column_intersections(folder_path):\n",
    "    datasets = {}\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.feather')]\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_feather(file_path)\n",
    "        datasets[filename] = set(df.columns)\n",
    "        print(f\"Processed: {filename} with {len(df.columns)} columns\")\n",
    "\n",
    "    dataset_names = list(datasets.keys())\n",
    "    dataset_columns = list(datasets.values())\n",
    "\n",
    "    common_columns = get_common_columns(dataset_columns)\n",
    "    all_columns = get_all_columns(dataset_columns)\n",
    "    \n",
    "    print(f\"\\nCommon Columns across all datasets: {len(common_columns)}\")\n",
    "    print(f\"Unique Columns across all datasets: {len(all_columns)}\")\n",
    "\n",
    "    with open('column_analysis.txt', 'w') as f:\n",
    "        f.write(f\"Common Columns across all datasets: {len(common_columns)}\\n\")\n",
    "        f.write(f\"Unique Columns across all datasets: {len(all_columns)}\\n\")\n",
    "        f.write(\"\\nList of common columns:\\n\")\n",
    "        f.write(\", \".join(common_columns))\n",
    "        f.write(\"\\n\\nList of all columns:\\n\")\n",
    "        f.write(\", \".join(all_columns))\n",
    "\n",
    "    overlap_matrix = pd.DataFrame(\n",
    "        index=dataset_names, columns=dataset_names, dtype=int\n",
    "    )\n",
    "    print(\"\\nCalculating column overlap...\")\n",
    "    \n",
    "    for (name1, cols1), (name2, cols2) in tqdm(itertools.combinations(datasets.items(), 2), \n",
    "                                               desc=\"Calculating overlaps\", unit=\"pair\"):\n",
    "        overlap = len(cols1.intersection(cols2))\n",
    "        overlap_matrix.loc[name1, name2] = overlap\n",
    "        overlap_matrix.loc[name2, name1] = overlap\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.title(\"Column Overlap between Datasets\")\n",
    "    plt.imshow(overlap_matrix.fillna(0), cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.colorbar(label=\"Number of Common Columns\")\n",
    "    plt.xticks(range(len(dataset_names)), dataset_names, rotation=90)\n",
    "    plt.yticks(range(len(dataset_names)), dataset_names)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('column_overlap.pdf')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\nAnalysis completed. Results saved as 'column_analysis.txt' and 'column_overlap.pdf'.\")\n",
    "    return overlap_matrix, common_columns, all_columns\n",
    "\n",
    "folder_path = '/home/vpal/hobotnica/All_datasets/data_feather_no_age_big_ds'\n",
    "overlap_matrix, common_columns, all_columns = analyze_column_intersections(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Diagrams Euler\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib_venn import venn2, venn3\n",
    "# from upsetplot import UpSet\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def get_column_sets(folder_path):\n",
    "#     datasets = {}\n",
    "#     column_counts = []\n",
    "    \n",
    "#     files = [f for f in os.listdir(folder_path) if f.endswith('.feather')]\n",
    "    \n",
    "#     print(\"Reading datasets...\")\n",
    "#     for filename in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         df = pd.read_feather(file_path)\n",
    "#         column_count = len(df.columns)\n",
    "#         datasets[filename] = set(df.columns)\n",
    "#         column_counts.append({'Dataset': filename, 'Number of Columns': column_count})\n",
    "    \n",
    "#     # Convert the column count data to a DataFrame\n",
    "#     column_counts_df = pd.DataFrame(column_counts)\n",
    "#     column_counts_df.sort_values(by='Number of Columns', ascending=True, inplace=True)\n",
    "    \n",
    "#     # Save the table to a text file\n",
    "#     column_counts_df.to_csv('column_counts.txt', index=False, sep='\\t')\n",
    "    \n",
    "#     print(\"\\nColumn counts saved to 'column_counts.txt'.\")\n",
    "#     return datasets, column_counts_df\n",
    "\n",
    "# def draw_venn_diagram(datasets):\n",
    "#     if len(datasets) == 2:\n",
    "#         names, sets = list(datasets.keys()), list(datasets.values())\n",
    "#         venn2(sets, set_labels=names)\n",
    "#         plt.title(\"Venn Diagram of Dataset Column Overlaps\")\n",
    "#         plt.show()\n",
    "#     elif len(datasets) == 3:\n",
    "#         names, sets = list(datasets.keys()), list(datasets.values())\n",
    "#         venn3(sets, set_labels=names)\n",
    "#         plt.title(\"Venn Diagram of Dataset Column Overlaps\")\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(\"Venn diagrams are only supported for 2 or 3 sets. Consider using an UpSet plot.\")\n",
    "\n",
    "# def draw_upset_plot(datasets, max_categories=20):\n",
    "#     \"\"\"\n",
    "#     Draw an UpSet plot for multiple datasets, limiting to the top `max_categories` intersections.\n",
    "#     \"\"\"\n",
    "#     from collections import defaultdict\n",
    "#     column_presence = defaultdict(list)\n",
    "    \n",
    "#     all_columns = set()\n",
    "#     for columns in datasets.values():\n",
    "#         all_columns.update(columns)\n",
    "    \n",
    "#     # Create a DataFrame where each row represents a column and each column is a dataset\n",
    "#     df = pd.DataFrame(\n",
    "#         {dataset: [col in columns for col in all_columns] \n",
    "#          for dataset, columns in datasets.items()},\n",
    "#         index=list(all_columns)\n",
    "#     )\n",
    "    \n",
    "#     # Group by the dataset columns to get intersection counts\n",
    "#     upset_data = df.groupby(list(datasets.keys())).size()\n",
    "    \n",
    "#     # Limit the data to the top `max_categories` intersections\n",
    "#     upset_data = upset_data.nlargest(max_categories)\n",
    "   \n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     upset = UpSet(upset_data)\n",
    "#     upset.plot()\n",
    "#     plt.title(\"UpSet Plot of Dataset Column Overlaps (Top Intersections)\")\n",
    "#     plt.savefig('upset_plot.pdf', format='pdf')\n",
    "#     plt.close()\n",
    "\n",
    "    \n",
    "# def visualize_overlaps(folder_path):\n",
    "#     datasets, column_counts_df = get_column_sets(folder_path)\n",
    "    \n",
    "#     # Display the sorted column counts table\n",
    "#     print(\"\\nTable of Column Counts:\")\n",
    "#     print(column_counts_df)\n",
    "    \n",
    "#     # Save the table to a text file\n",
    "#     with open('column_counts_sorted.txt', 'w') as f:\n",
    "#         f.write(column_counts_df.to_string(index=False))\n",
    "    \n",
    "#     if len(datasets) <= 3:\n",
    "#         draw_venn_diagram(datasets)\n",
    "#     else:\n",
    "#         draw_upset_plot(datasets)\n",
    "\n",
    "# folder_path = '/home/vpal/hobotnica/All_datasets/data_feather_no_age_big_ds'\n",
    "# visualize_overlaps(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows removed from all files in the folder.\n"
     ]
    }
   ],
   "source": [
    "# ##removing repeats from txt files triplets\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# folder_path = '/home/vpal/hobotnica/All_datasets/H_scores_triplets_its_txt_format_not_feather'\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.feather'):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         data = pd.read_csv(file_path, delimiter='\\t') \n",
    "#         data = data.drop_duplicates()\n",
    "#         data.to_csv(file_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save ds to feather format\n",
    "import pandas as pd\n",
    "import os\n",
    "directory = \"/home/vpal/hobotnica/All_datasets/no_age_multiple_regression_big_datasets\"\n",
    "output_path = \"/home/vpal/hobotnica/All_datasets/data_feather\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)  \n",
    "    data = pd.read_pickle(file_path)\n",
    "    base_name = os.path.splitext(file)[0]\n",
    "    output_file_name = base_name + \".feather\"\n",
    "    output_file_path = os.path.join(output_path, output_file_name)\n",
    "    data.to_feather(output_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the final version of result table with H-score and pval\n",
    "file1_path = '/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_H_scores_no_age.csv' #I change the name of files after result of the code\n",
    "file2_path = '/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_H_scores_with_age_only.csv' #I change the name of files after result of the code\n",
    "\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "df12 = pd.merge(df2, df1, on=\"Dataset_ID\", how='outer')\n",
    "df12 = df12.rename(columns={\"H_score_x\" :\"H_score\", \"H_score_y\" :\"H_score_no_age\", \"p_value_x\": \"p_value\", \"p_value_y\": \"p_value_no_age\"})\n",
    "\n",
    "df3 = pd.read_csv(\"/home/vpal/hobotnica/ds_info_for_res.csv\")\n",
    "final_df = pd.merge(df3, df12, on=\"Dataset ID\", how='outer')\n",
    "\n",
    "final_df.to_csv('/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##round h_scores\n",
    "columns_to_round = ['H_score', 'H_score_no_age'] \n",
    "final_df = pd.read_csv(\"/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv\")\n",
    "final_df[\"H_score\"] = final_df[\"H_score\"].round(3)\n",
    "final_df[\"H_score_no_age\"] = final_df[\"H_score_no_age\"].round(3)\n",
    "final_df.to_csv('/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting by column\n",
    "final_df = pd.read_csv(\"/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv\")\n",
    "final_df = final_df.sort_values(by = \"H_score_no_age\", ascending=False) \n",
    "final_df.to_csv('/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Multiple linear regression with intercept subtraction\n",
    "\n",
    "# def substract_age(file_path):\n",
    "#     data = pd.read_csv(file_path)\n",
    "#     data_1 = data.copy()\n",
    "\n",
    "#     data_1['Condition'] = data_1['Condition'].apply(lambda x: 0 if x == 'HC' else 1)\n",
    "    \n",
    "#     for site in data_1.columns[1:-3]:\n",
    "#         y_train = data_1[site] \n",
    "#         X_train = data_1[['Age', 'Condition']] \n",
    "\n",
    "#         model = LinearRegression()\n",
    "#         model.fit(X_train, y_train)\n",
    "        \n",
    "#         age_coef = model.coef_[0] # Coefficient b1\n",
    "#         intercept = model.intercept_\n",
    "#         data_1[site] = data_1[site] - (age_coef * data_1['Age'] + intercept)\n",
    "        \n",
    "#     data_1[\"Condition\"] = data[\"Condition\"]\n",
    "    \n",
    "#     return data_1\n",
    "\n",
    "# datasets = '/tank/projects/vpalagina_hobotnica/hobotnica/clocks/GrimAgeV1/datasets_GrimAgeV1'\n",
    "# PhenoAge_mult_regr_age = '/tank/projects/vpalagina_hobotnica/hobotnica/clocks/GrimAgeV1/no_age'\n",
    "\n",
    "# for filename in os.listdir(datasets):\n",
    "#     file_path = os.path.join(datasets, filename)\n",
    "#     processed_df = substract_age(file_path)               \n",
    "#     output_path = os.path.join(PhenoAge_mult_regr_age, filename)\n",
    "#     processed_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cudf\n",
    "# import cudf\n",
    "# import pandas as pd\n",
    "# from cuml import LinearRegression\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def substract_age(file_path):\n",
    "#     data = pd.read_pickle(file_path)\n",
    "#     data = cudf.DataFrame.from_pandas(data)\n",
    "#     data_1 = data.copy()\n",
    "    \n",
    "#     data_1['Condition'] = data_1['Condition'].applymap(lambda x: 0 if x == 'HC' else 1)\n",
    "\n",
    "#     for idx, site in tqdm(enumerate(data_1.columns[1:-2])):\n",
    "#         y_train = data_1[site]\n",
    "#         X_train = data_1[['Age', 'Condition']]\n",
    "\n",
    "#         model = LinearRegression()\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         age_coef = model.coef_[0]\n",
    "#         intercept = model.intercept_\n",
    "#         data_1[site] = data_1[site] - (age_coef * data_1['Age'] + intercept)\n",
    "        \n",
    "#         # every 1000th site name for progress tracking\n",
    "#         if (idx + 1) % 1000 == 0:\n",
    "#             print(f'Processed site: {site}')\n",
    "            \n",
    "#     data_1[\"Condition\"] = data[\"Condition\"]\n",
    "    \n",
    "#     return data_1\n",
    "\n",
    "# def process_and_save(file_path, output_folder):\n",
    "#     print(f'Starting processing of dataset: {os.path.basename(file_path)}')\n",
    "#     output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "\n",
    "#     if os.path.exists(output_path):\n",
    "#         print(f'Skipping already processed file: {output_path}')\n",
    "#         return\n",
    "    \n",
    "#     # Save\n",
    "#     processed_df = substract_age(file_path)\n",
    "#     processed_df = processed_df.to_pandas()\n",
    "#     processed_df.to_pickle(output_path)\n",
    "#     print(f'Processed and saved: {output_path}')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     initial_datasets = '/home/vpal/hobotnica/All_datasets/data_imputed_with_meta_for_regression_pkl'\n",
    "#     substracted_age = '/home/vpal/hobotnica/All_datasets/gpu'\n",
    "\n",
    "#     files = [os.path.join(initial_datasets, filename) for filename in os.listdir(initial_datasets) if filename.endswith('.pickle')]\n",
    "\n",
    "#     for file_path in tqdm(files):\n",
    "#         process_and_save(file_path, substracted_age)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hobotnica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
