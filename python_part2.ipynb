{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering valid files...\n",
      "Filtering valid files: 100%|██████████| 41/41 [06:10<00:00,  9.03s/it]\n",
      "Finding common columns across valid files...\n",
      "Finding common columns: 100%|██████████| 39/39 [06:14<00:00,  9.60s/it]\n",
      "Processing and saving files...\n",
      "Processing files:   0%|          | 0/39 [00:00<?, ?it/s]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE134429.feather\n",
      "Processing files:   3%|▎         | 1/39 [00:30<19:25, 30.68s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE131752.feather\n",
      "Processing files:   5%|▌         | 2/39 [01:01<19:03, 30.90s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE87640.feather\n",
      "Processing files:   8%|▊         | 3/39 [01:27<16:59, 28.33s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE217633.feather\n",
      "Processing files:  10%|█         | 4/39 [02:01<17:53, 30.67s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE87648.feather\n",
      "Processing files:  13%|█▎        | 5/39 [02:27<16:22, 28.90s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE81961.feather\n",
      "Processing files:  15%|█▌        | 6/39 [02:52<15:14, 27.70s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE59685.feather\n",
      "Processing files:  18%|█▊        | 7/39 [03:17<14:24, 27.01s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE144858.feather\n",
      "Processing files:  21%|██        | 8/39 [03:42<13:29, 26.13s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE99624.feather\n",
      "Processing files:  23%|██▎       | 9/39 [04:07<12:58, 25.95s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE118468.feather\n",
      "Processing files:  26%|██▌       | 10/39 [04:32<12:22, 25.59s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE106648.feather\n",
      "Processing files:  28%|██▊       | 11/39 [04:59<12:06, 25.94s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE111223.feather\n",
      "Processing files:  31%|███       | 12/39 [05:25<11:40, 25.93s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE67751.feather\n",
      "Processing files:  33%|███▎      | 13/39 [05:51<11:13, 25.91s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE219293.feather\n",
      "Processing files:  36%|███▌      | 14/39 [06:22<11:29, 27.59s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE122244.feather\n",
      "Processing files:  38%|███▊      | 15/39 [06:54<11:31, 28.80s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE130029.feather\n",
      "Processing files:  41%|████      | 16/39 [07:19<10:37, 27.73s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE77696.feather\n",
      "Processing files:  44%|████▎     | 17/39 [07:45<09:59, 27.24s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE130030.feather\n",
      "Processing files:  46%|████▌     | 18/39 [08:11<09:23, 26.83s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE42861.feather\n",
      "Processing files:  49%|████▊     | 19/39 [08:38<08:57, 26.90s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE193836.feather\n",
      "Processing files:  51%|█████▏    | 20/39 [09:02<08:17, 26.19s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE71841.feather\n",
      "Processing files:  54%|█████▍    | 21/39 [09:27<07:41, 25.64s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE145714.feather\n",
      "Processing files:  56%|█████▋    | 22/39 [09:59<07:47, 27.51s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE72776.feather\n",
      "Processing files:  59%|█████▉    | 23/39 [10:24<07:11, 26.98s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE72338.feather\n",
      "Processing files:  62%|██████▏   | 24/39 [10:49<06:34, 26.31s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE72774.feather\n",
      "Processing files:  64%|██████▍   | 25/39 [11:17<06:14, 26.72s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE175364.feather\n",
      "Processing files:  67%|██████▋   | 26/39 [11:42<05:39, 26.13s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE118469.feather\n",
      "Processing files:  69%|██████▉   | 27/39 [12:07<05:11, 25.93s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE166611.feather\n",
      "Processing files:  72%|███████▏  | 28/39 [12:31<04:37, 25.20s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE131989.feather\n",
      "Processing files:  74%|███████▍  | 29/39 [12:56<04:13, 25.34s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE67705.feather\n",
      "Processing files:  77%|███████▋  | 30/39 [13:23<03:50, 25.62s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE32148.feather\n",
      "Processing files:  79%|███████▉  | 31/39 [13:48<03:23, 25.47s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE182991.feather\n",
      "Processing files:  82%|████████▏ | 32/39 [14:19<03:09, 27.13s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE111629.feather\n",
      "Processing files:  85%|████████▍ | 33/39 [14:46<02:43, 27.30s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE56581.feather\n",
      "Processing files:  87%|████████▋ | 34/39 [15:12<02:13, 26.71s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE107143.feather\n",
      "Processing files:  90%|████████▉ | 35/39 [15:37<01:44, 26.23s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE56046.feather\n",
      "Processing files:  92%|█████████▏| 36/39 [16:07<01:22, 27.34s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE156994.feather\n",
      "Processing files:  95%|█████████▍| 37/39 [16:32<00:53, 26.72s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE143942.feather\n",
      "Processing files:  97%|█████████▋| 38/39 [16:57<00:26, 26.10s/it]Processed and saved: /home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception/GSE214297.feather\n",
      "Processing files: 100%|██████████| 39/39 [17:23<00:00, 26.75s/it]\n"
     ]
    }
   ],
   "source": [
    "###here I remove too small datasets with 27k sites and filter the remain data to include only those sites which located in all datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def process_feather_files(input_folder, output_folder, min_columns=28000):\n",
    "    feather_files = [f for f in os.listdir(input_folder) if f.endswith('.feather')]\n",
    "    valid_files = []\n",
    "    \n",
    "    print(\"Filtering valid files...\")\n",
    "    for file in tqdm(feather_files, desc=\"Filtering valid files\", file=sys.stdout):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        df = pd.read_feather(file_path)\n",
    "        if df.shape[1] >= min_columns:\n",
    "            valid_files.append(file_path)\n",
    "\n",
    "    if not valid_files:\n",
    "        print(\"No valid files found with the minimum column requirement.\")\n",
    "        return\n",
    "\n",
    "    common_columns = None\n",
    "    print(\"Finding common columns across valid files...\")\n",
    "    for file_path in tqdm(valid_files, desc=\"Finding common columns\", file=sys.stdout):\n",
    "        df = pd.read_feather(file_path)\n",
    "        if common_columns is None:\n",
    "            common_columns = set(df.columns)\n",
    "        else:\n",
    "            common_columns.intersection_update(df.columns)\n",
    "\n",
    "    if not common_columns:\n",
    "        print(\"No common columns found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\"Processing and saving files...\")\n",
    "    for file_path in tqdm(valid_files, desc=\"Processing files\", file=sys.stdout):\n",
    "        df = pd.read_feather(file_path)\n",
    "        df_common = df[list(common_columns)]\n",
    "        \n",
    "        output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "        df_common.to_feather(output_path)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "input_folder = \"/home/vpal/hobotnica/All_datasets/datasets/data_feather_no_age_big_ds\"\n",
    "output_folder = \"/home/vpal/hobotnica/All_datasets/datasets/feather_ds_no_age_39_ds_only_interception\"\n",
    "process_feather_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed GSE118469, current unique sites: 170691\n",
      "Processed GSE59685, current unique sites: 238824\n",
      "Processed GSE111629, current unique sites: 296757\n",
      "Processed GSE134429, current unique sites: 301989\n",
      "Processed GSE32148, current unique sites: 307124\n",
      "Processed GSE118468, current unique sites: 317649\n",
      "Processed GSE56581, current unique sites: 318620\n",
      "Processed GSE130030, current unique sites: 321960\n",
      "Processed GSE214297, current unique sites: 323269\n",
      "Processed GSE42861, current unique sites: 323698\n",
      "Processed GSE193836, current unique sites: 328408\n",
      "Processed GSE87640, current unique sites: 328413\n",
      "Processed GSE143942, current unique sites: 328414\n",
      "Processed GSE107143, current unique sites: 332075\n",
      "Processed GSE56606, current unique sites: 332184\n",
      "Processed GSE77696, current unique sites: 335328\n",
      "Processed GSE72338, current unique sites: 335916\n",
      "Processed GSE106648, current unique sites: 335916\n",
      "Processed GSE72774, current unique sites: 335966\n",
      "Processed GSE99624, current unique sites: 336403\n",
      "Processed GSE87648, current unique sites: 337740\n",
      "Processed GSE217633, current unique sites: 342569\n",
      "Processed GSE175364, current unique sites: 342569\n",
      "Processed GSE145714, current unique sites: 342718\n",
      "Processed GSE81961, current unique sites: 342718\n",
      "Processed GSE56046, current unique sites: 342718\n",
      "Processed GSE122244, current unique sites: 343868\n",
      "Processed GSE131752, current unique sites: 343980\n",
      "Processed GSE49909, current unique sites: 344071\n",
      "Processed GSE111223, current unique sites: 344099\n",
      "Processed GSE67705, current unique sites: 344099\n",
      "Processed GSE144858, current unique sites: 344649\n",
      "Processed GSE67751, current unique sites: 344649\n",
      "Processed GSE130029, current unique sites: 344972\n",
      "Processed GSE71841, current unique sites: 344972\n",
      "Processed GSE156994, current unique sites: 345497\n",
      "Processed GSE219293, current unique sites: 345573\n",
      "Processed GSE72776, current unique sites: 345601\n",
      "Processed GSE131989, current unique sites: 346877\n",
      "Processed GSE182991, current unique sites: 346924\n",
      "Processed GSE166611, current unique sites: 347252\n",
      "Common sites found in at least 70.0% of datasets saved to /home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/common_sites_threshold.txt\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def extract_unique_sites(file_path):\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path, sep='\\t', usecols=[0, 1, 2], engine='c', low_memory=False)\n",
    "#         return set(df.iloc[:, 0]).union(set(df.iloc[:, 1]), set(df.iloc[:, 2]))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\")\n",
    "#         return set()\n",
    "\n",
    "# def find_common_sites(folder_path, output_file, threshold=1.0):\n",
    "#     files = [f for f in os.listdir(folder_path)]\n",
    "\n",
    "#     site_counts = {}\n",
    "#     total_files = len(files)\n",
    "\n",
    "#     for filename in files:\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         unique_sites = extract_unique_sites(file_path)\n",
    "        \n",
    "#         for site in unique_sites:\n",
    "#             if site in site_counts:\n",
    "#                 site_counts[site] += 1\n",
    "#             else:\n",
    "#                 site_counts[site] = 1\n",
    "\n",
    "#         print(f\"Processed {filename}, current unique sites: {len(site_counts)}\")\n",
    "\n",
    "#     min_occurrences = int(threshold * total_files)\n",
    "\n",
    "#     common_sites = [site for site, count in site_counts.items() if count >= min_occurrences]\n",
    "#     if common_sites:\n",
    "#         with open(output_file, 'w') as f:\n",
    "#             for site in sorted(common_sites):\n",
    "#                 f.write(f\"{site}\\n\")\n",
    "#         print(f\"Common sites found in at least {threshold*100}% of datasets saved to {output_file}\")\n",
    "#     else:\n",
    "#         print(f\"No common sites found in at least {threshold*100}% of datasets.\")\n",
    "\n",
    "# folder_path = '/home/vpal/hobotnica/All_datasets/triplets_h_score_bigger_than_0.5'\n",
    "# output_file = '/home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/common_sites_threshold.txt'\n",
    "\n",
    "# find_common_sites(folder_path, output_file, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def visualize_overlap_matrix(csv_file, output_image):\n",
    "#     \"\"\"Visualize the overlap matrix as a heatmap without annotations.\"\"\"\n",
    "#     # Load the overlap matrix from the CSV file\n",
    "#     overlap_matrix = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "#     # Create a heatmap using Seaborn without annotations\n",
    "#     plt.figure(figsize=(18, 15))\n",
    "#     sns.heatmap(\n",
    "#         overlap_matrix, \n",
    "#         annot=False,    # Disable annotations\n",
    "#         cmap='viridis', \n",
    "#         linewidths=0.3, \n",
    "#         square=True, \n",
    "#         cbar_kws={'label': 'Number of Common Columns'}\n",
    "#     )\n",
    "    \n",
    "#     # Add titles and labels\n",
    "#     plt.title(\"Column Overlap Between Datasets\", fontsize=16)\n",
    "#     plt.xlabel(\"Datasets\")\n",
    "#     plt.ylabel(\"Datasets\")\n",
    "    \n",
    "#     # Rotate x-axis labels for better readability\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.yticks(rotation=0)\n",
    "    \n",
    "#     # Save the heatmap as an image file\n",
    "#     plt.savefig(output_image, format='png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "# # Define the paths to your CSV file and the output image\n",
    "# csv_file = '/home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/overlap_matrix.csv'\n",
    "# output_image = '/home/vpal/hobotnica/All_datasets/analysis_of_triplets_res/overlap_heatmap_no_numbers.png'\n",
    "\n",
    "# # Visualize the overlap matrix\n",
    "# visualize_overlap_matrix(csv_file, output_image)\n",
    "# print(f\"Heatmap saved as {output_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def count_site_occurrences(folder_path):\n",
    "#     site_counts = defaultdict(int)\n",
    "\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         df = pd.read_csv(file_path, sep='\\t', engine='python')\n",
    "\n",
    "#         all_sites = set(df['Column1']).union(set(df['Column2']), set(df['Column3']))\n",
    "#         for site in all_sites:\n",
    "#             site_counts[site] += 1\n",
    "\n",
    "#     return site_counts\n",
    "\n",
    "# def filter_rows_by_site_count(folder_path, site_counts, min_threshold=20):\n",
    "\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         df = pd.read_csv(file_path, sep='\\t', engine='python')\n",
    "        \n",
    "#         filtered_df = df[\n",
    "#             (df['Column1'].apply(lambda x: site_counts[x] >= min_threshold)) &\n",
    "#             (df['Column2'].apply(lambda x: site_counts[x] >= min_threshold)) &\n",
    "#             (df['Column3'].apply(lambda x: site_counts[x] >= min_threshold))\n",
    "#         ]\n",
    "\n",
    "#         filtered_df.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "# folder_path = '/home/vpal/hobotnica/All_datasets/triplets_h_score_bigger_than_0.5'\n",
    "\n",
    "# site_counts = count_site_occurrences(folder_path)\n",
    "\n",
    "# filter_rows_by_site_count(folder_path, site_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed GSE118469, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE118469\n",
      "Processed GSE59685, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE59685\n",
      "Processed GSE111629, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE111629\n",
      "Processed GSE134429, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE134429\n",
      "Processed GSE32148, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE32148\n",
      "Processed GSE118468, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE118468\n",
      "Processed GSE56581, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE56581\n",
      "Processed GSE130030, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE130030\n",
      "Processed GSE214297, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE214297\n",
      "Processed GSE42861, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE42861\n",
      "Processed GSE193836, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE193836\n",
      "Processed GSE87640, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE87640\n",
      "Processed GSE143942, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE143942\n",
      "Processed GSE107143, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE107143\n",
      "Processed GSE56606, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE56606\n",
      "Processed GSE77696, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE77696\n",
      "Processed GSE72338, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE72338\n",
      "Processed GSE106648, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE106648\n",
      "Processed GSE72774, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE72774\n",
      "Processed GSE99624, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE99624\n",
      "Processed GSE87648, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE87648\n",
      "Processed GSE217633, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE217633\n",
      "Processed GSE175364, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE175364\n",
      "Processed GSE145714, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE145714\n",
      "Processed GSE81961, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE81961\n",
      "Processed GSE56046, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE56046\n",
      "Processed GSE122244, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE122244\n",
      "Processed GSE131752, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE131752\n",
      "Processed GSE49909, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE49909\n",
      "Processed GSE111223, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE111223\n",
      "Processed GSE67705, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE67705\n",
      "Processed GSE144858, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE144858\n",
      "Processed GSE67751, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE67751\n",
      "Processed GSE130029, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE130029\n",
      "Processed GSE71841, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE71841\n",
      "Processed GSE156994, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE156994\n",
      "Processed GSE219293, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE219293\n",
      "Processed GSE72776, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE72776\n",
      "Processed GSE131989, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE131989\n",
      "Processed GSE182991, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE182991\n",
      "Processed GSE166611, saved to /home/vpal/hobotnica/All_datasets/triplets_h_score_bigger/GSE166611\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def filter_files(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all .txt files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Read the file into a DataFrame\n",
    "        df = pd.read_csv(input_path, sep='\\s+', engine='python')\n",
    "        \n",
    "        # Filter out rows where the h-score (4th column) is less than 0.5\n",
    "        filtered_df = df[df['Result'] >= 0.5]\n",
    "        \n",
    "        # Save the filtered DataFrame to the output folder\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        filtered_df.to_csv(output_path, sep='\\t', index=False)\n",
    "        print(f\"Processed {filename}, saved to {output_path}\")\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = '/home/vpal/hobotnica/All_datasets/H_scores_triplets'\n",
    "output_folder = '/home/vpal/hobotnica/All_datasets/triplets_h_score_bigger'\n",
    "\n",
    "# Run the function\n",
    "filter_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/41 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|▏         | 1/41 [00:13<09:14, 13.87s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE134429.feather with 766019 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   5%|▍         | 2/41 [00:28<09:17, 14.31s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE131752.feather with 816982 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 3/41 [00:37<07:29, 11.84s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE87640.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  10%|▉         | 4/41 [00:53<08:21, 13.54s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE217633.feather with 835168 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  12%|█▏        | 5/41 [01:02<07:05, 11.82s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE87648.feather with 460305 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  15%|█▍        | 6/41 [01:10<06:15, 10.72s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE81961.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  17%|█▋        | 7/41 [01:19<05:42, 10.07s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE59685.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|█▉        | 8/41 [01:27<05:06,  9.28s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE144858.feather with 410891 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  22%|██▏       | 9/41 [01:35<04:50,  9.09s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE99624.feather with 485478 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  24%|██▍       | 10/41 [01:44<04:39,  9.00s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE118468.feather with 485479 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  27%|██▋       | 11/41 [01:53<04:30,  9.02s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE106648.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  29%|██▉       | 12/41 [02:02<04:21,  9.02s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE111223.feather with 485479 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  32%|███▏      | 13/41 [02:11<04:11,  8.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE67751.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  34%|███▍      | 14/41 [02:27<04:56, 10.99s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE219293.feather with 865768 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  37%|███▋      | 15/41 [02:42<05:19, 12.28s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE122244.feather with 822800 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  39%|███▉      | 16/41 [02:51<04:41, 11.25s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE130029.feather with 469664 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  41%|████▏     | 17/41 [03:00<04:15, 10.64s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE77696.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  44%|████▍     | 18/41 [03:09<03:51, 10.06s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE130030.feather with 471338 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  46%|████▋     | 19/41 [03:21<03:54, 10.65s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE42861.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  49%|████▉     | 20/41 [03:29<03:25,  9.80s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE193836.feather with 414249 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  51%|█████     | 21/41 [03:37<03:09,  9.46s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE71841.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  54%|█████▎    | 22/41 [03:53<03:37, 11.43s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE145714.feather with 865999 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  56%|█████▌    | 23/41 [04:02<03:11, 10.64s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE72776.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  59%|█████▊    | 24/41 [04:11<02:50, 10.04s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE72338.feather with 485476 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  61%|██████    | 25/41 [04:20<02:38,  9.88s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE72774.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  63%|██████▎   | 26/41 [04:29<02:23,  9.59s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE175364.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  66%|██████▌   | 27/41 [04:38<02:11,  9.38s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE118469.feather with 485481 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  68%|██████▊   | 28/41 [04:46<01:54,  8.84s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE166611.feather with 409889 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  71%|███████   | 29/41 [04:55<01:46,  8.87s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE131989.feather with 407514 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  73%|███████▎  | 30/41 [05:04<01:38,  8.94s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE67705.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  76%|███████▌  | 31/41 [05:13<01:29,  8.94s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE32148.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  78%|███████▊  | 32/41 [05:28<01:37, 10.85s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE182991.feather with 825171 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  80%|████████  | 33/41 [05:40<01:28, 11.07s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE111629.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  83%|████████▎ | 34/41 [05:49<01:13, 10.46s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE56581.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  85%|████████▌ | 35/41 [05:49<00:45,  7.51s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE56606.feather with 27580 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 36/41 [05:57<00:38,  7.74s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE107143.feather with 460297 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|█████████ | 37/41 [06:08<00:34,  8.58s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE56046.feather with 485482 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  93%|█████████▎| 38/41 [06:16<00:25,  8.54s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE156994.feather with 403356 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▌| 39/41 [06:17<00:12,  6.17s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE49909.feather with 27580 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  98%|█████████▊| 40/41 [06:26<00:06,  7.00s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE143942.feather with 485417 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 41/41 [06:36<00:00,  9.66s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: GSE214297.feather with 518476 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Columns across all datasets: 15300\n",
      "Unique Columns across all datasets: 900354\n",
      "\n",
      "Calculating column overlap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating overlaps: 820pair [01:01, 13.26pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis completed. Results saved as 'column_analysis.txt' and 'column_overlap.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_common_columns(datasets):\n",
    "    common_columns = set(datasets[0])\n",
    "    for columns in datasets[1:]:\n",
    "        common_columns.intersection_update(columns)\n",
    "    return common_columns\n",
    "\n",
    "def get_all_columns(datasets):\n",
    "    all_columns = set()\n",
    "    for columns in datasets:\n",
    "        all_columns.update(columns)\n",
    "    return all_columns\n",
    "\n",
    "def analyze_column_intersections(folder_path):\n",
    "    datasets = {}\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.feather')]\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_feather(file_path)\n",
    "        datasets[filename] = set(df.columns)\n",
    "        print(f\"Processed: {filename} with {len(df.columns)} columns\")\n",
    "\n",
    "    dataset_names = list(datasets.keys())\n",
    "    dataset_columns = list(datasets.values())\n",
    "\n",
    "    common_columns = get_common_columns(dataset_columns)\n",
    "    all_columns = get_all_columns(dataset_columns)\n",
    "    \n",
    "    print(f\"\\nCommon Columns across all datasets: {len(common_columns)}\")\n",
    "    print(f\"Unique Columns across all datasets: {len(all_columns)}\")\n",
    "\n",
    "    with open('column_analysis.txt', 'w') as f:\n",
    "        f.write(f\"Common Columns across all datasets: {len(common_columns)}\\n\")\n",
    "        f.write(f\"Unique Columns across all datasets: {len(all_columns)}\\n\")\n",
    "        f.write(\"\\nList of common columns:\\n\")\n",
    "        f.write(\", \".join(common_columns))\n",
    "        f.write(\"\\n\\nList of all columns:\\n\")\n",
    "        f.write(\", \".join(all_columns))\n",
    "\n",
    "    overlap_matrix = pd.DataFrame(\n",
    "        index=dataset_names, columns=dataset_names, dtype=int\n",
    "    )\n",
    "    print(\"\\nCalculating column overlap...\")\n",
    "    \n",
    "    for (name1, cols1), (name2, cols2) in tqdm(itertools.combinations(datasets.items(), 2), \n",
    "                                               desc=\"Calculating overlaps\", unit=\"pair\"):\n",
    "        overlap = len(cols1.intersection(cols2))\n",
    "        overlap_matrix.loc[name1, name2] = overlap\n",
    "        overlap_matrix.loc[name2, name1] = overlap\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.title(\"Column Overlap between Datasets\")\n",
    "    plt.imshow(overlap_matrix.fillna(0), cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.colorbar(label=\"Number of Common Columns\")\n",
    "    plt.xticks(range(len(dataset_names)), dataset_names, rotation=90)\n",
    "    plt.yticks(range(len(dataset_names)), dataset_names)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('column_overlap.pdf')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\nAnalysis completed. Results saved as 'column_analysis.txt' and 'column_overlap.pdf'.\")\n",
    "    return overlap_matrix, common_columns, all_columns\n",
    "\n",
    "folder_path = '/home/vpal/hobotnica/All_datasets/data_feather_no_age_big_ds'\n",
    "overlap_matrix, common_columns, all_columns = analyze_column_intersections(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 41/41 [06:23<00:00,  9.35s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column counts saved to 'column_counts.txt'.\n",
      "\n",
      "Table of Column Counts:\n",
      "              Dataset  Number of Columns\n",
      "38   GSE49909.feather              27580\n",
      "34   GSE56606.feather              27580\n",
      "37  GSE156994.feather             403356\n",
      "28  GSE131989.feather             407514\n",
      "27  GSE166611.feather             409889\n",
      "7   GSE144858.feather             410891\n",
      "19  GSE193836.feather             414249\n",
      "35  GSE107143.feather             460297\n",
      "4    GSE87648.feather             460305\n",
      "15  GSE130029.feather             469664\n",
      "17  GSE130030.feather             471338\n",
      "24   GSE72774.feather             485417\n",
      "22   GSE72776.feather             485417\n",
      "16   GSE77696.feather             485417\n",
      "25  GSE175364.feather             485417\n",
      "5    GSE81961.feather             485417\n",
      "10  GSE106648.feather             485417\n",
      "12   GSE67751.feather             485417\n",
      "32  GSE111629.feather             485417\n",
      "39  GSE143942.feather             485417\n",
      "29   GSE67705.feather             485417\n",
      "23   GSE72338.feather             485476\n",
      "8    GSE99624.feather             485478\n",
      "11  GSE111223.feather             485479\n",
      "9   GSE118468.feather             485479\n",
      "26  GSE118469.feather             485481\n",
      "20   GSE71841.feather             485482\n",
      "33   GSE56581.feather             485482\n",
      "30   GSE32148.feather             485482\n",
      "6    GSE59685.feather             485482\n",
      "2    GSE87640.feather             485482\n",
      "18   GSE42861.feather             485482\n",
      "36   GSE56046.feather             485482\n",
      "40  GSE214297.feather             518476\n",
      "0   GSE134429.feather             766019\n",
      "1   GSE131752.feather             816982\n",
      "14  GSE122244.feather             822800\n",
      "31  GSE182991.feather             825171\n",
      "3   GSE217633.feather             835168\n",
      "13  GSE219293.feather             865768\n",
      "21  GSE145714.feather             865999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpal/miniconda3/envs/hobotnica/lib/python3.10/site-packages/upsetplot/plotting.py:795: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  styles[\"linewidth\"].fillna(1, inplace=True)\n",
      "/home/vpal/miniconda3/envs/hobotnica/lib/python3.10/site-packages/upsetplot/plotting.py:796: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  styles[\"facecolor\"].fillna(self._facecolor, inplace=True)\n",
      "/home/vpal/miniconda3/envs/hobotnica/lib/python3.10/site-packages/upsetplot/plotting.py:797: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  styles[\"edgecolor\"].fillna(styles[\"facecolor\"], inplace=True)\n",
      "/home/vpal/miniconda3/envs/hobotnica/lib/python3.10/site-packages/upsetplot/plotting.py:798: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  styles[\"linestyle\"].fillna(\"solid\", inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Diagrams Euler\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn3\n",
    "from upsetplot import UpSet\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_column_sets(folder_path):\n",
    "    datasets = {}\n",
    "    column_counts = []\n",
    "    \n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.feather')]\n",
    "    \n",
    "    print(\"Reading datasets...\")\n",
    "    for filename in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_feather(file_path)\n",
    "        column_count = len(df.columns)\n",
    "        datasets[filename] = set(df.columns)\n",
    "        column_counts.append({'Dataset': filename, 'Number of Columns': column_count})\n",
    "    \n",
    "    # Convert the column count data to a DataFrame\n",
    "    column_counts_df = pd.DataFrame(column_counts)\n",
    "    column_counts_df.sort_values(by='Number of Columns', ascending=True, inplace=True)\n",
    "    \n",
    "    # Save the table to a text file\n",
    "    column_counts_df.to_csv('column_counts.txt', index=False, sep='\\t')\n",
    "    \n",
    "    print(\"\\nColumn counts saved to 'column_counts.txt'.\")\n",
    "    return datasets, column_counts_df\n",
    "\n",
    "def draw_venn_diagram(datasets):\n",
    "    if len(datasets) == 2:\n",
    "        names, sets = list(datasets.keys()), list(datasets.values())\n",
    "        venn2(sets, set_labels=names)\n",
    "        plt.title(\"Venn Diagram of Dataset Column Overlaps\")\n",
    "        plt.show()\n",
    "    elif len(datasets) == 3:\n",
    "        names, sets = list(datasets.keys()), list(datasets.values())\n",
    "        venn3(sets, set_labels=names)\n",
    "        plt.title(\"Venn Diagram of Dataset Column Overlaps\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Venn diagrams are only supported for 2 or 3 sets. Consider using an UpSet plot.\")\n",
    "\n",
    "def draw_upset_plot(datasets, max_categories=20):\n",
    "    \"\"\"\n",
    "    Draw an UpSet plot for multiple datasets, limiting to the top `max_categories` intersections.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    column_presence = defaultdict(list)\n",
    "    \n",
    "    all_columns = set()\n",
    "    for columns in datasets.values():\n",
    "        all_columns.update(columns)\n",
    "    \n",
    "    # Create a DataFrame where each row represents a column and each column is a dataset\n",
    "    df = pd.DataFrame(\n",
    "        {dataset: [col in columns for col in all_columns] \n",
    "         for dataset, columns in datasets.items()},\n",
    "        index=list(all_columns)\n",
    "    )\n",
    "    \n",
    "    # Group by the dataset columns to get intersection counts\n",
    "    upset_data = df.groupby(list(datasets.keys())).size()\n",
    "    \n",
    "    # Limit the data to the top `max_categories` intersections\n",
    "    upset_data = upset_data.nlargest(max_categories)\n",
    "   \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    upset = UpSet(upset_data)\n",
    "    upset.plot()\n",
    "    plt.title(\"UpSet Plot of Dataset Column Overlaps (Top Intersections)\")\n",
    "    plt.savefig('upset_plot.pdf', format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def visualize_overlaps(folder_path):\n",
    "    datasets, column_counts_df = get_column_sets(folder_path)\n",
    "    \n",
    "    # Display the sorted column counts table\n",
    "    print(\"\\nTable of Column Counts:\")\n",
    "    print(column_counts_df)\n",
    "    \n",
    "    # Save the table to a text file\n",
    "    with open('column_counts_sorted.txt', 'w') as f:\n",
    "        f.write(column_counts_df.to_string(index=False))\n",
    "    \n",
    "    if len(datasets) <= 3:\n",
    "        draw_venn_diagram(datasets)\n",
    "    else:\n",
    "        draw_upset_plot(datasets)\n",
    "\n",
    "folder_path = '/home/vpal/hobotnica/All_datasets/data_feather_no_age_big_ds'\n",
    "visualize_overlaps(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows removed from all files in the folder.\n"
     ]
    }
   ],
   "source": [
    "# ##removing repeats from txt files triplets\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# folder_path = '/home/vpal/hobotnica/All_datasets/H_scores_triplets_its_txt_format_not_feather'\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.feather'):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         data = pd.read_csv(file_path, delimiter='\\t') \n",
    "#         data = data.drop_duplicates()\n",
    "#         data.to_csv(file_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save ds to feather format\n",
    "import pandas as pd\n",
    "import os\n",
    "directory = \"/home/vpal/hobotnica/All_datasets/no_age_multiple_regression_big_datasets\"\n",
    "output_path = \"/home/vpal/hobotnica/All_datasets/data_feather\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)  \n",
    "    data = pd.read_pickle(file_path)\n",
    "    base_name = os.path.splitext(file)[0]\n",
    "    output_file_name = base_name + \".feather\"\n",
    "    output_file_path = os.path.join(output_path, output_file_name)\n",
    "    data.to_feather(output_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg00000029</th>\n",
       "      <th>cg00000108</th>\n",
       "      <th>cg00000109</th>\n",
       "      <th>cg00000165</th>\n",
       "      <th>cg00000236</th>\n",
       "      <th>cg00000289</th>\n",
       "      <th>cg00000292</th>\n",
       "      <th>cg00000321</th>\n",
       "      <th>cg00000363</th>\n",
       "      <th>cg00000622</th>\n",
       "      <th>...</th>\n",
       "      <th>rs845016</th>\n",
       "      <th>rs877309</th>\n",
       "      <th>rs9292570</th>\n",
       "      <th>rs9363764</th>\n",
       "      <th>rs939290</th>\n",
       "      <th>rs951295</th>\n",
       "      <th>rs966367</th>\n",
       "      <th>rs9839873</th>\n",
       "      <th>Age</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM796660</th>\n",
       "      <td>0.53599</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>0.008980</td>\n",
       "      <td>-0.007031</td>\n",
       "      <td>0.019418</td>\n",
       "      <td>-0.009883</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>0.080755</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>-0.004720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205647</td>\n",
       "      <td>-0.128459</td>\n",
       "      <td>0.402958</td>\n",
       "      <td>-0.253294</td>\n",
       "      <td>0.329365</td>\n",
       "      <td>-0.523574</td>\n",
       "      <td>0.019631</td>\n",
       "      <td>-0.242880</td>\n",
       "      <td>49.0</td>\n",
       "      <td>IBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM796661</th>\n",
       "      <td>0.51288</td>\n",
       "      <td>-0.006587</td>\n",
       "      <td>-0.023730</td>\n",
       "      <td>-0.011871</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>-0.011343</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.079055</td>\n",
       "      <td>0.013317</td>\n",
       "      <td>-0.000920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183677</td>\n",
       "      <td>-0.144759</td>\n",
       "      <td>0.408058</td>\n",
       "      <td>-0.268534</td>\n",
       "      <td>0.324175</td>\n",
       "      <td>-0.519394</td>\n",
       "      <td>0.027311</td>\n",
       "      <td>-0.247880</td>\n",
       "      <td>49.0</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM796662</th>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>-0.036311</td>\n",
       "      <td>-0.011212</td>\n",
       "      <td>-0.029440</td>\n",
       "      <td>0.013014</td>\n",
       "      <td>-0.002194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160531</td>\n",
       "      <td>-0.104124</td>\n",
       "      <td>0.389309</td>\n",
       "      <td>0.154945</td>\n",
       "      <td>-0.085205</td>\n",
       "      <td>-0.075325</td>\n",
       "      <td>0.037765</td>\n",
       "      <td>0.073119</td>\n",
       "      <td>43.0</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM796663</th>\n",
       "      <td>0.53280</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.030899</td>\n",
       "      <td>-0.031370</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.012881</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.019346</td>\n",
       "      <td>-0.003804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149141</td>\n",
       "      <td>-0.101384</td>\n",
       "      <td>0.394419</td>\n",
       "      <td>0.160985</td>\n",
       "      <td>-0.067395</td>\n",
       "      <td>-0.076075</td>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.071229</td>\n",
       "      <td>43.0</td>\n",
       "      <td>IBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM796664</th>\n",
       "      <td>0.52627</td>\n",
       "      <td>0.007764</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.027179</td>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.019614</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148640</td>\n",
       "      <td>-0.112996</td>\n",
       "      <td>-0.039201</td>\n",
       "      <td>0.166252</td>\n",
       "      <td>-0.093555</td>\n",
       "      <td>0.339350</td>\n",
       "      <td>0.014639</td>\n",
       "      <td>0.079778</td>\n",
       "      <td>44.0</td>\n",
       "      <td>IBD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 485482 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cg00000029  cg00000108  cg00000109  cg00000165  cg00000236  \\\n",
       "index                                                                   \n",
       "GSM796660     0.53599    0.008493    0.008980   -0.007031    0.019418   \n",
       "GSM796661     0.51288   -0.006587   -0.023730   -0.011871    0.000708   \n",
       "GSM796662     0.50556    0.001125   -0.003001   -0.007460    0.008722   \n",
       "GSM796663     0.53280    0.012225    0.030899   -0.031370   -0.001148   \n",
       "GSM796664     0.52627    0.007764    0.014278    0.004905    0.014036   \n",
       "\n",
       "           cg00000289  cg00000292  cg00000321  cg00000363  cg00000622  ...  \\\n",
       "index                                                                  ...   \n",
       "GSM796660   -0.009883   -0.001469    0.080755    0.007417   -0.004720  ...   \n",
       "GSM796661   -0.011343    0.011091    0.079055    0.013317   -0.000920  ...   \n",
       "GSM796662   -0.036311   -0.011212   -0.029440    0.013014   -0.002194  ...   \n",
       "GSM796663   -0.012881    0.009818   -0.000790   -0.019346   -0.003804  ...   \n",
       "GSM796664    0.027179   -0.005523   -0.019614    0.016598    0.002229  ...   \n",
       "\n",
       "           rs845016  rs877309  rs9292570  rs9363764  rs939290  rs951295  \\\n",
       "index                                                                     \n",
       "GSM796660 -0.205647 -0.128459   0.402958  -0.253294  0.329365 -0.523574   \n",
       "GSM796661 -0.183677 -0.144759   0.408058  -0.268534  0.324175 -0.519394   \n",
       "GSM796662 -0.160531 -0.104124   0.389309   0.154945 -0.085205 -0.075325   \n",
       "GSM796663 -0.149141 -0.101384   0.394419   0.160985 -0.067395 -0.076075   \n",
       "GSM796664 -0.148640 -0.112996  -0.039201   0.166252 -0.093555  0.339350   \n",
       "\n",
       "           rs966367  rs9839873   Age  Condition  \n",
       "index                                            \n",
       "GSM796660  0.019631  -0.242880  49.0        IBD  \n",
       "GSM796661  0.027311  -0.247880  49.0         HC  \n",
       "GSM796662  0.037765   0.073119  43.0         HC  \n",
       "GSM796663  0.018545   0.071229  43.0        IBD  \n",
       "GSM796664  0.014639   0.079778  44.0        IBD  \n",
       "\n",
       "[5 rows x 485482 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.read_feather(\"/home/vpal/hobotnica/All_datasets/data_feather/GSE32148.feather\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the final version of result table with H-score and pval\n",
    "file1_path = '/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_H_scores_no_age.csv' #I change the name of files after result of the code\n",
    "file2_path = '/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_H_scores_with_age_only.csv' #I change the name of files after result of the code\n",
    "\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "df12 = pd.merge(df2, df1, on=\"Dataset_ID\", how='outer')\n",
    "df12 = df12.rename(columns={\"H_score_x\" :\"H_score\", \"H_score_y\" :\"H_score_no_age\", \"p_value_x\": \"p_value\", \"p_value_y\": \"p_value_no_age\"})\n",
    "\n",
    "df3 = pd.read_csv(\"/home/vpal/hobotnica/ds_info_for_res.csv\")\n",
    "final_df = pd.merge(df3, df12, on=\"Dataset ID\", how='outer')\n",
    "\n",
    "final_df.to_csv('/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##round h_scores\n",
    "columns_to_round = ['H_score', 'H_score_no_age'] \n",
    "final_df = pd.read_csv(\"/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv\")\n",
    "final_df[\"H_score\"] = final_df[\"H_score\"].round(3)\n",
    "final_df[\"H_score_no_age\"] = final_df[\"H_score_no_age\"].round(3)\n",
    "final_df.to_csv('/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting by column\n",
    "final_df = pd.read_csv(\"/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv\")\n",
    "final_df = final_df.sort_values(by = \"H_score_no_age\", ascending=False) \n",
    "final_df.to_csv('/home/vpal/hobotnica/PhenoAgeV2_res_imputed_rand_signature/PhenoAgeV2_final_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/tank/projects/vpalagina_hobotnica/hobotnica/datasets_PhenoAgeV2/GSE42861\")\n",
    "plt.scatter(data[\"Age\"], data[[\"cg00056497\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "def adjust_for_age(data, site_column):\n",
    "    \n",
    "    X = data['Age']\n",
    "    X = sm.add_constant(X)  # Add intercept\n",
    "    y = data[site_column]\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()  # Fit linear regression model\n",
    "    predictions = model.predict(X)  # Predicted values\n",
    "    \n",
    "    # Subtract the predicted values from the actual site values\n",
    "    adjusted_values = y - predictions\n",
    "    \n",
    "    return adjusted_values\n",
    "\n",
    "def process_files_in_folder(input_folder, output_folder):\n",
    "\n",
    "    # Iterate over each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        adjusted_data = data.copy()\n",
    "        \n",
    "        # Adjust each site column for age\n",
    "        for site in data.columns[1:-2]:  # Exclude 'Age' and \"Condition\" and \"samples\" columns\n",
    "            adjusted_data[site] = adjust_for_age(data, site)\n",
    "        \n",
    "        # Save the adjusted data to a new CSV file in the output folder\n",
    "        output_file_path = os.path.join(output_folder, filename)\n",
    "        adjusted_data.to_csv(output_file_path, index=False)\n",
    "        print(f\"Processed and saved: {output_file_path}\")\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = '/tank/projects/vpalagina_hobotnica/hobotnica/datasets_PhenoAgeV2_noNA_new'\n",
    "output_folder = '/tank/projects/vpalagina_hobotnica/hobotnica/substracted_age_Pheno_another_approach'\n",
    "\n",
    "# Process all files in the input folder\n",
    "process_files_in_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Multiple linear regression with intercept subtraction\n",
    "\n",
    "def substract_age(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data_1 = data.copy()\n",
    "\n",
    "    data_1['Condition'] = data_1['Condition'].apply(lambda x: 0 if x == 'HC' else 1)\n",
    "    \n",
    "    for site in data_1.columns[1:-3]:\n",
    "        y_train = data_1[site] \n",
    "        X_train = data_1[['Age', 'Condition']] \n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        age_coef = model.coef_[0] # Coefficient b1\n",
    "        intercept = model.intercept_\n",
    "        data_1[site] = data_1[site] - (age_coef * data_1['Age'] + intercept)\n",
    "        \n",
    "    data_1[\"Condition\"] = data[\"Condition\"]\n",
    "    \n",
    "    return data_1\n",
    "\n",
    "datasets = '/tank/projects/vpalagina_hobotnica/hobotnica/clocks/GrimAgeV1/datasets_GrimAgeV1'\n",
    "PhenoAge_mult_regr_age = '/tank/projects/vpalagina_hobotnica/hobotnica/clocks/GrimAgeV1/no_age'\n",
    "\n",
    "for filename in os.listdir(datasets):\n",
    "    file_path = os.path.join(datasets, filename)\n",
    "    processed_df = substract_age(file_path)               \n",
    "    output_path = os.path.join(PhenoAge_mult_regr_age, filename)\n",
    "    processed_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of dataset: GSE72776.pickle\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/home/vpal/miniconda3/envs/rapids_env/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(initial_datasets, filename) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(initial_datasets) \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mprocess_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubstracted_age\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mprocess_and_save\u001b[0;34m(file_path, output_folder)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43msubstract_age\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m processed_df\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     45\u001b[0m processed_df\u001b[38;5;241m.\u001b[39mto_pickle(output_path)\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36msubstract_age\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstract_age\u001b[39m(file_path):\n\u001b[0;32m----> 9\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m cudf\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_pandas(data)\n\u001b[1;32m     11\u001b[0m     data_1 \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids_env/lib/python3.8/site-packages/pandas/io/pickle.py:208\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids_env/lib/python3.8/site-packages/pandas/compat/pickle_compat.py:249\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[1;32m    246\u001b[0m         up \u001b[38;5;241m=\u001b[39m Unpickler(fh)\n\u001b[1;32m    247\u001b[0m     up\u001b[38;5;241m.\u001b[39mis_verbose \u001b[38;5;241m=\u001b[39m is_verbose\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids_env/lib/python3.8/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids_env/lib/python3.8/pickle.py:1537\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids_env/lib/python3.8/site-packages/pandas/compat/pickle_compat.py:189\u001b[0m, in \u001b[0;36mUnpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    187\u001b[0m key \u001b[38;5;241m=\u001b[39m (module, name)\n\u001b[1;32m    188\u001b[0m module, name \u001b[38;5;241m=\u001b[39m _class_locations_map\u001b[38;5;241m.\u001b[39mget(key, key)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids_env/lib/python3.8/pickle.py:1581\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m-> 1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids_env/lib/python3.8/pickle.py:331\u001b[0m, in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(name, obj)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/home/vpal/miniconda3/envs/rapids_env/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cudf\n",
    "import cudf\n",
    "import pandas as pd\n",
    "from cuml import LinearRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "def substract_age(file_path):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    data = cudf.DataFrame.from_pandas(data)\n",
    "    data_1 = data.copy()\n",
    "    \n",
    "    data_1['Condition'] = data_1['Condition'].applymap(lambda x: 0 if x == 'HC' else 1)\n",
    "\n",
    "    for idx, site in tqdm(enumerate(data_1.columns[1:-2])):\n",
    "        y_train = data_1[site]\n",
    "        X_train = data_1[['Age', 'Condition']]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        age_coef = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        data_1[site] = data_1[site] - (age_coef * data_1['Age'] + intercept)\n",
    "        \n",
    "        # every 1000th site name for progress tracking\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f'Processed site: {site}')\n",
    "            \n",
    "    data_1[\"Condition\"] = data[\"Condition\"]\n",
    "    \n",
    "    return data_1\n",
    "\n",
    "def process_and_save(file_path, output_folder):\n",
    "    print(f'Starting processing of dataset: {os.path.basename(file_path)}')\n",
    "    output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        print(f'Skipping already processed file: {output_path}')\n",
    "        return\n",
    "    \n",
    "    # Save\n",
    "    processed_df = substract_age(file_path)\n",
    "    processed_df = processed_df.to_pandas()\n",
    "    processed_df.to_pickle(output_path)\n",
    "    print(f'Processed and saved: {output_path}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    initial_datasets = '/home/vpal/hobotnica/All_datasets/data_imputed_with_meta_for_regression_pkl'\n",
    "    substracted_age = '/home/vpal/hobotnica/All_datasets/gpu'\n",
    "\n",
    "    files = [os.path.join(initial_datasets, filename) for filename in os.listdir(initial_datasets) if filename.endswith('.pickle')]\n",
    "\n",
    "    for file_path in tqdm(files):\n",
    "        process_and_save(file_path, substracted_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.core.multiarray' has no attribute 'unsignedinteger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/pandas/io/pickle.py:202\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    201\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/numpy/_core/numeric.py:12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numerictypes \u001b[38;5;28;01mas\u001b[39;00m nt\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ALLOW_THREADS, BUFSIZE, CLIP, MAXDIMS, MAY_SHARE_BOUNDS, MAY_SHARE_EXACT,\n\u001b[1;32m     15\u001b[0m     RAISE, WRAP, arange, array, asarray, asanyarray, ascontiguousarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     _get_promotion_state, _set_promotion_state\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/numpy/_core/numerictypes.py:102\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_string_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     99\u001b[0m     english_lower, english_upper, english_capitalize, LOWER_TABLE, UPPER_TABLE\n\u001b[1;32m    100\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_type_aliases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    103\u001b[0m     sctypeDict, allTypes, sctypes\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _kind_name\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/numpy/_core/_type_aliases.py:38\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _abstract_type_name \u001b[38;5;129;01min\u001b[39;00m _abstract_type_names:\n\u001b[0;32m---> 38\u001b[0m     allTypes[_abstract_type_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_abstract_type_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m typeinfo\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.core.multiarray' has no attribute 'unsignedinteger'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m pickle_files:\n\u001b[1;32m     11\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, file_name)\n\u001b[0;32m---> 12\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     14\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, file_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/pandas/io/pickle.py:207\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/pandas/compat/pickle_compat.py:231\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# \"Unpickler\" has no attribute \"is_verbose\"  [attr-defined]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     up\u001b[38;5;241m.\u001b[39mis_verbose \u001b[38;5;241m=\u001b[39m is_verbose  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/pickle.py:1538\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/pandas/compat/pickle_compat.py:162\u001b[0m, in \u001b[0;36mUnpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    160\u001b[0m key \u001b[38;5;241m=\u001b[39m (module, name)\n\u001b[1;32m    161\u001b[0m module, name \u001b[38;5;241m=\u001b[39m _class_locations_map\u001b[38;5;241m.\u001b[39mget(key, key)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/pickle.py:1580\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING:\n\u001b[1;32m   1579\u001b[0m         module \u001b[38;5;241m=\u001b[39m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[0;32m-> 1580\u001b[0m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/numpy/_core/numeric.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numerictypes \u001b[38;5;28;01mas\u001b[39;00m nt\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ALLOW_THREADS, BUFSIZE, CLIP, MAXDIMS, MAY_SHARE_BOUNDS, MAY_SHARE_EXACT,\n\u001b[1;32m     15\u001b[0m     RAISE, WRAP, arange, array, asarray, asanyarray, ascontiguousarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     _get_promotion_state, _set_promotion_state\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/numpy/_core/numerictypes.py:102\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# we don't need all these imports, but we need to keep them for compatibility\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# for users using np._core.numerictypes.UPPER_TABLE\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_string_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     99\u001b[0m     english_lower, english_upper, english_capitalize, LOWER_TABLE, UPPER_TABLE\n\u001b[1;32m    100\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_type_aliases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    103\u001b[0m     sctypeDict, allTypes, sctypes\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _kind_name\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# we don't export these for import *, but we do want them accessible\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# as numerictypes.bool, etc.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hobotnica/lib/python3.10/site-packages/numpy/_core/_type_aliases.py:38\u001b[0m\n\u001b[1;32m     31\u001b[0m _abstract_type_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minexact\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflexible\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharacter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplexfloating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsignedinteger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignedinteger\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _abstract_type_name \u001b[38;5;129;01min\u001b[39;00m _abstract_type_names:\n\u001b[0;32m---> 38\u001b[0m     allTypes[_abstract_type_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_abstract_type_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m typeinfo\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNPY_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m c_names_dict:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.core.multiarray' has no attribute 'unsignedinteger'"
     ]
    }
   ],
   "source": [
    "###pickle_to_parquet\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = '/home/vpal/hobotnica/All_datasets/data_imputed_with_meta_for_regression_pkl'\n",
    "output_dir = '/home/vpal/hobotnica/All_datasets/parquet_files'\n",
    "pickle_files = [f for f in os.listdir(input_dir)]\n",
    "\n",
    "for file_name in pickle_files:\n",
    "    file_path = os.path.join(input_dir, file_name)\n",
    "    data = pd.read_pickle(file_path)\n",
    "    data = data.reset_index()\n",
    "    output_file = os.path.join(output_dir, file_name.replace('.pickle', '.parquet'))\n",
    "    data.to_parquet(output_file, index=True)\n",
    "    print(f'Converted {file_name} to Parquet format.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hobotnica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
